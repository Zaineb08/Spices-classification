{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbb79c4d",
   "metadata": {},
   "source": [
    "# Mod√®le 1: CNN Custom avec Explainability\n",
    "\n",
    "**Architecture CNN personnalis√©e avec:**\n",
    "- 4 blocs convolutionnels (3‚Üí64‚Üí128‚Üí256‚Üí512 canaux)\n",
    "- Batch Normalization pour stabilit√©\n",
    "- Dropout pour r√©gularisation\n",
    "- Global Average Pooling\n",
    "- Grad-CAM pour l'explicabilit√©\n",
    "\n",
    "**Dataset:** 2200 images (11 classes d'√©pices)  \n",
    "**Split:** 70% train / 15% val / 15% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df40b35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñ•Ô∏è  Device: cpu\n",
      "   PyTorch version: 2.10.0+cpu\n",
      "   CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "# ====================================\n",
    "# IMPORTS\n",
    "# ====================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import cv2\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuration device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üñ•Ô∏è  Device: {device}\")\n",
    "print(f\"   PyTorch version: {torch.__version__}\")\n",
    "print(f\"   CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95bce2f",
   "metadata": {},
   "source": [
    "## 1. Dataset et DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153a0c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset class d√©finie\n"
     ]
    }
   ],
   "source": [
    "# ====================================\n",
    "# DATASET CLASS\n",
    "# ====================================\n",
    "class SpiceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset personnalis√© pour les √©pices.\n",
    "    Charge les images depuis la structure: root_dir/class_name/*.jpg\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # R√©cup√©rer les noms de classes (dossiers)\n",
    "        self.class_names = sorted([d.name for d in self.root_dir.iterdir() if d.is_dir()])\n",
    "        self.class_to_idx = {name: idx for idx, name in enumerate(self.class_names)}\n",
    "        \n",
    "        # Charger tous les chemins d'images\n",
    "        for class_name in self.class_names:\n",
    "            class_dir = self.root_dir / class_name\n",
    "            for img_path in class_dir.glob('*.jpg'):\n",
    "                self.images.append(img_path)\n",
    "                self.labels.append(self.class_to_idx[class_name])\n",
    "        \n",
    "        print(f\"   ‚úÖ Loaded {len(self.images)} images from {root_dir}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Charger l'image\n",
    "        img_path = self.images[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Appliquer les transformations\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "print(\"‚úÖ Dataset class d√©finie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb448ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã Configuration des transformations d'images...\n",
      "\n",
      "   ‚úÖ Train transform: 10 augmentations\n",
      "   ‚úÖ Val/Test transform: Normalisation seulement\n"
     ]
    }
   ],
   "source": [
    "# ====================================\n",
    "# TRANSFORMATIONS (AUGMENTATION)\n",
    "# ====================================\n",
    "print(\"\\nüìã Configuration des transformations d'images...\\n\")\n",
    "\n",
    "# Train: Augmentation aggressive\n",
    "train_transform = transforms.Compose([\n",
    "    # 1. G√©om√©triques (sur PIL Image)\n",
    "    transforms.Resize((224, 224), interpolation=transforms.InterpolationMode.LANCZOS),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.3),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    \n",
    "    # 2. Couleur/Lumi√®re (sur PIL Image)\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.RandomAutocontrast(p=0.3),\n",
    "    transforms.RandomAdjustSharpness(sharpness_factor=2, p=0.3),\n",
    "    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 0.5)),\n",
    "    \n",
    "    # 3. Conversion PIL ‚Üí Tensor (CRUCIAL!)\n",
    "    transforms.ToTensor(),\n",
    "    \n",
    "    # 4. Augmentation sur Tensor\n",
    "    transforms.RandomErasing(p=0.2, scale=(0.02, 0.1)),\n",
    "    \n",
    "    # 5. Normalisation ImageNet\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Val/Test: Pas d'augmentation\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224), interpolation=transforms.InterpolationMode.LANCZOS),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "print(\"   ‚úÖ Train transform: 10 augmentations\")\n",
    "print(\"   ‚úÖ Val/Test transform: Normalisation seulement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c85d60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÇ Chargement des datasets...\n",
      "\n",
      "   ‚úÖ Loaded 1540 images from ../dataset/splits/train\n",
      "   ‚úÖ Loaded 330 images from ../dataset/splits/val\n",
      "   ‚úÖ Loaded 330 images from ../dataset/splits/test\n",
      "\n",
      "   üìä Classes (11): ['anis', 'cannelle', 'carvi', 'clou_girofle', 'cubebe', 'cumin', 'curcuma', 'gingembre', 'paprika', 'poivre noir', 'safran']\n",
      "   üìä Train: 1540 images\n",
      "   üìä Val: 330 images\n",
      "   üìä Test: 330 images\n",
      "\n",
      "   ‚úÖ Batch size: 32\n",
      "   ‚úÖ Num workers: 2 (for faster data loading)\n",
      "   ‚úÖ Train batches: 49\n",
      "   ‚úÖ Val batches: 11\n",
      "   ‚úÖ Test batches: 11\n",
      "\n",
      "üîç V√©rification du format des donn√©es...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zaineb\\.conda\\envs\\spices_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1118: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    }
   ],
   "source": [
    "# ====================================\n",
    "# CR√âATION DES DATASETS ET DATALOADERS\n",
    "# ====================================\n",
    "print(\"\\nüìÇ Chargement des datasets...\\n\")\n",
    "\n",
    "# Cr√©er les datasets\n",
    "train_dataset = SpiceDataset('../dataset/splits/train', transform=train_transform)\n",
    "val_dataset = SpiceDataset('../dataset/splits/val', transform=val_transform)\n",
    "test_dataset = SpiceDataset('../dataset/splits/test', transform=val_transform)\n",
    "\n",
    "print(f\"\\n   üìä Classes ({len(train_dataset.class_names)}): {train_dataset.class_names}\")\n",
    "print(f\"   üìä Train: {len(train_dataset)} images\")\n",
    "print(f\"   üìä Val: {len(val_dataset)} images\")\n",
    "print(f\"   üìä Test: {len(test_dataset)} images\")\n",
    "\n",
    "# Cr√©er les data loaders\n",
    "batch_size = 32\n",
    "num_workers = 2 \n",
    "# Only use pin_memory if a GPU is available\n",
    "pin_memory = torch.cuda.is_available()\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n",
    "                         num_workers=num_workers, pin_memory=pin_memory)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, \n",
    "                       num_workers=num_workers, pin_memory=pin_memory)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, \n",
    "                        num_workers=num_workers, pin_memory=pin_memory)\n",
    "\n",
    "print(f\"\\n   ‚úÖ Batch size: {batch_size}\")\n",
    "print(f\"   ‚úÖ Num workers: {num_workers} (for faster data loading)\")\n",
    "print(f\"   ‚úÖ Pin memory: {pin_memory} (speeds up CPU-to-GPU transfer)\")\n",
    "print(f\"   ‚úÖ Train batches: {len(train_loader)}\")\n",
    "print(f\"   ‚úÖ Val batches: {len(val_loader)}\")\n",
    "print(f\"   ‚úÖ Test batches: {len(test_loader)}\")\n",
    "\n",
    "# V√©rification du format de sortie\n",
    "print(\"\\nüîç V√©rification du format des donn√©es...\")\n",
    "sample_batch, sample_labels = next(iter(train_loader))\n",
    "print(f\"   ‚úÖ Batch shape: {sample_batch.shape} (batch, channels, height, width)\")\n",
    "print(f\"   ‚úÖ Batch dtype: {sample_batch.dtype}\")\n",
    "print(f\"   ‚úÖ Batch range: [{sample_batch.min():.3f}, {sample_batch.max():.3f}]\")\n",
    "print(f\"   ‚úÖ Labels shape: {sample_labels.shape}\")\n",
    "assert sample_batch.shape[1:] == (3, 224, 224), \"Shape incorrecte!\"\n",
    "print(\"\\n‚úÖ‚úÖ‚úÖ Format des donn√©es VALID√â!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35636311",
   "metadata": {},
   "source": [
    "## 2. Architecture CNN Custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79864152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# ARCHITECTURE CNN CUSTOM\n",
    "# ====================================\n",
    "class CustomCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN Custom √† 4 blocs avec architecture progressive:\n",
    "    - Block 1: 3 ‚Üí 64 canaux (d√©tection features simples)\n",
    "    - Block 2: 64 ‚Üí 128 canaux (combinaison features)\n",
    "    - Block 3: 128 ‚Üí 256 canaux (patterns locaux)\n",
    "    - Block 4: 256 ‚Üí 512 canaux (patterns globaux)\n",
    "    - Global Average Pooling\n",
    "    - Classifier: 512 ‚Üí 256 ‚Üí 11 classes\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=11):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        \n",
    "        # Block 1: 3 ‚Üí 64 (d√©tection ar√™tes, couleurs)\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # 224√ó224 ‚Üí 112√ó112\n",
    "        )\n",
    "        \n",
    "        # Block 2: 64 ‚Üí 128 (formes locales)\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # 112√ó112 ‚Üí 56√ó56\n",
    "        )\n",
    "        \n",
    "        # Block 3: 128 ‚Üí 256 (objets locaux)\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # 56√ó56 ‚Üí 28√ó28\n",
    "        )\n",
    "        \n",
    "        # Block 4: 256 ‚Üí 512 (structure globale)\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # 28√ó28 ‚Üí 14√ó14\n",
    "        )\n",
    "        \n",
    "        # Global Average Pooling: 14√ó14√ó512 ‚Üí 512\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        # Classifier avec r√©gularisation forte\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Variables pour Grad-CAM\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        \n",
    "        # Sauvegarder activations pour Grad-CAM\n",
    "        if x.requires_grad:\n",
    "            x.register_hook(self.save_gradient)\n",
    "        self.activations = x\n",
    "        \n",
    "        # Global Average Pooling + Classifier\n",
    "        x = self.gap(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "    def save_gradient(self, grad):\n",
    "        \"\"\"Hook pour sauvegarder les gradients (Grad-CAM)\"\"\"\n",
    "        self.gradients = grad\n",
    "\n",
    "\n",
    "# Cr√©er le mod√®le\n",
    "print(\"\\nüèóÔ∏è  Cr√©ation du mod√®le CNN Custom...\\n\")\n",
    "model = CustomCNN(num_classes=11).to(device)\n",
    "\n",
    "# Compter les param√®tres\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(model)\n",
    "print(f\"\\nüìä Statistiques du mod√®le:\")\n",
    "print(f\"   Total param√®tres: {total_params:,}\")\n",
    "print(f\"   Param√®tres entra√Ænables: {trainable_params:,}\")\n",
    "print(f\"   Taille estim√©e: ~{total_params * 4 / 1024 / 1024:.1f} MB (float32)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9dd8b7",
   "metadata": {},
   "source": [
    "## 3. Entra√Ænement du Mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010ce1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# CONFIGURATION ENTRA√éNEMENT\n",
    "# ====================================\n",
    "print(\"\\n‚öôÔ∏è  Configuration de l'entra√Ænement...\\n\")\n",
    "\n",
    "# Loss, Optimizer, Scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', patience=3, factor=0.5\n",
    ")\n",
    "\n",
    "# OPTIMIZATION: Add GradScaler for Mixed Precision\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Hyperparam√®tres\n",
    "num_epochs = 30\n",
    "best_val_acc = 0\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': []\n",
    "}\n",
    "\n",
    "print(f\"   ‚úÖ Loss: CrossEntropyLoss\")\n",
    "print(f\"   ‚úÖ Optimizer: Adam (lr=0.001)\")\n",
    "print(f\"   ‚úÖ Scheduler: ReduceLROnPlateau (patience=3, factor=0.5)\")\n",
    "print(f\"   ‚úÖ Mixed Precision: Enabled (via GradScaler)\")\n",
    "print(f\"   ‚úÖ Epochs: {num_epochs}\")\n",
    "print(f\"   ‚úÖ Best model will be saved to: model_cnn_custom_best.pth\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26a08b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# FONCTIONS D'ENTRA√éNEMENT\n",
    "# ====================================\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, scaler, device):\n",
    "    \"\"\"Entra√Æner le mod√®le sur une epoch avec Mixed Precision\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc='Training', leave=False)\n",
    "    for images, labels in pbar:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass with autocast\n",
    "        with autocast():\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass with scaler\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        # M√©triques\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'acc': f'{100. * correct / total:.2f}%'\n",
    "        })\n",
    "    \n",
    "    epoch_loss = running_loss / len(loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    \"\"\"√âvaluer le mod√®le sur validation/test set avec Mixed Precision\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc='Validation', leave=False):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass with autocast\n",
    "            with autocast():\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            # M√©triques\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "print(\"‚úÖ Fonctions d'entra√Ænement d√©finies (avec Mixed Precision)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e0c928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# BOUCLE D'ENTRA√éNEMENT\n",
    "# ====================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üî• D√âBUT DE L'ENTRA√éNEMENT\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üìç Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Train (pass scaler)\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, scaler, device)\n",
    "    \n",
    "    # Validation\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Sauvegarder l'historique\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    # Afficher les r√©sultats\n",
    "    print(f\"\\nüìä R√©sultats Epoch {epoch+1}:\")\n",
    "    print(f\"   Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"   Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.2f}%\")\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler.step(val_loss)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"   Learning Rate: {current_lr:.6f}\")\n",
    "    \n",
    "    # Sauvegarder le meilleur mod√®le\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_acc': val_acc,\n",
    "            'val_loss': val_loss,\n",
    "            'train_acc': train_acc,\n",
    "            'train_loss': train_loss,\n",
    "            'class_names': train_dataset.class_names\n",
    "        }, '../models/model_cnn_custom_best.pth')\n",
    "        print(f\"   ‚úÖ Nouveau meilleur mod√®le sauvegard√©! Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"‚úÖ ENTRA√éNEMENT TERMIN√â!\")\n",
    "print(f\"   Meilleure Val Accuracy: {best_val_acc:.2f}%\")\n",
    "print(\"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5c7e25",
   "metadata": {},
   "source": [
    "## 4. Visualisation des R√©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1193e3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# VISUALISATION LOSS & ACCURACY\n",
    "# ====================================\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2, marker='o', markersize=4)\n",
    "axes[0].plot(history['val_loss'], label='Val Loss', linewidth=2, marker='s', markersize=4)\n",
    "axes[0].set_title('Loss Evolution', fontsize=16, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(alpha=0.3, linestyle='--')\n",
    "\n",
    "# Accuracy plot\n",
    "axes[1].plot(history['train_acc'], label='Train Accuracy', linewidth=2, marker='o', markersize=4)\n",
    "axes[1].plot(history['val_acc'], label='Val Accuracy', linewidth=2, marker='s', markersize=4)\n",
    "axes[1].set_title('Accuracy Evolution', fontsize=16, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(alpha=0.3, linestyle='--')\n",
    "axes[1].axhline(y=best_val_acc, color='red', linestyle=':', label=f'Best: {best_val_acc:.2f}%', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../models/training_curves_cnn_custom.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Graphiques sauvegard√©s: ../models/training_curves_cnn_custom.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de0bfac",
   "metadata": {},
   "source": [
    "## 5. √âvaluation sur le Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9509bd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# √âVALUATION SUR TEST SET\n",
    "# ====================================\n",
    "print(\"\\nüìä √âvaluation sur le Test Set...\\n\")\n",
    "\n",
    "# Charger le meilleur mod√®le\n",
    "checkpoint = torch.load('model_cnn_custom_best.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"‚úÖ Meilleur mod√®le charg√© (Epoch {checkpoint['epoch']}, Val Acc: {checkpoint['val_acc']:.2f}%)\\n\")\n",
    "\n",
    "# √âvaluation\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader, desc='Testing'):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        _, predicted = outputs.max(1)\n",
    "        \n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Accuracy\n",
    "test_acc = 100. * np.sum(np.array(all_preds) == np.array(all_labels)) / len(all_labels)\n",
    "print(f\"\\nüéØ Test Accuracy: {test_acc:.2f}%\\n\")\n",
    "\n",
    "# Classification Report\n",
    "print(\"=\"*70)\n",
    "print(\"üìã CLASSIFICATION REPORT\")\n",
    "print(\"=\"*70)\n",
    "print(classification_report(\n",
    "    all_labels, all_preds, \n",
    "    target_names=train_dataset.class_names,\n",
    "    digits=4\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f25638a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# MATRICE DE CONFUSION\n",
    "# ====================================\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    cm, annot=True, fmt='d', cmap='Blues',\n",
    "    xticklabels=train_dataset.class_names,\n",
    "    yticklabels=train_dataset.class_names,\n",
    "    cbar_kws={'label': 'Count'}\n",
    ")\n",
    "plt.title('Confusion Matrix - CNN Custom', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.ylabel('True Label', fontsize=13)\n",
    "plt.xlabel('Predicted Label', fontsize=13)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../models/confusion_matrix_cnn_custom.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Matrice de confusion sauvegard√©e: ../models/confusion_matrix_cnn_custom.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa117f00",
   "metadata": {},
   "source": [
    "## 6. Explainability: Grad-CAM\n",
    "\n",
    "Grad-CAM (Gradient-weighted Class Activation Mapping) permet de visualiser quelles r√©gions de l'image sont importantes pour la pr√©diction du mod√®le."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dc9bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# GRAD-CAM IMPLEMENTATION\n",
    "# ====================================\n",
    "def generate_gradcam(model, image, target_class):\n",
    "    \"\"\"\n",
    "    G√©n√®re une heatmap Grad-CAM pour une image et une classe cible.\n",
    "    \n",
    "    Args:\n",
    "        model: Le mod√®le CNN\n",
    "        image: Image tensor (C, H, W)\n",
    "        target_class: Classe cible (int)\n",
    "    \n",
    "    Returns:\n",
    "        cam: Heatmap Grad-CAM (H, W)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    image = image.unsqueeze(0).to(device)\n",
    "    image.requires_grad = True\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(image)\n",
    "    \n",
    "    # Backward pass sur la classe cible\n",
    "    model.zero_grad()\n",
    "    output[0, target_class].backward()\n",
    "    \n",
    "    # R√©cup√©rer gradients et activations\n",
    "    gradients = model.gradients.cpu().data.numpy()[0]  # (512, 14, 14)\n",
    "    activations = model.activations.cpu().data.numpy()[0]  # (512, 14, 14)\n",
    "    \n",
    "    # Calculer les poids (global average pooling des gradients)\n",
    "    weights = np.mean(gradients, axis=(1, 2))  # (512,)\n",
    "    \n",
    "    # Calculer la CAM (weighted sum des activations)\n",
    "    cam = np.zeros(activations.shape[1:], dtype=np.float32)  # (14, 14)\n",
    "    for i, w in enumerate(weights):\n",
    "        cam += w * activations[i]\n",
    "    \n",
    "    # ReLU (garder seulement les contributions positives)\n",
    "    cam = np.maximum(cam, 0)\n",
    "    \n",
    "    # Normaliser entre 0 et 1\n",
    "    if cam.max() > 0:\n",
    "        cam = cam / cam.max()\n",
    "    \n",
    "    # Redimensionner √† la taille de l'image originale\n",
    "    cam = cv2.resize(cam, (224, 224))\n",
    "    \n",
    "    return cam\n",
    "\n",
    "\n",
    "def denormalize_image(tensor):\n",
    "    \"\"\"D√©normalise un tensor d'image (inverse ImageNet normalization)\"\"\"\n",
    "    img = tensor.cpu().numpy().transpose(1, 2, 0)\n",
    "    img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "    img = np.clip(img, 0, 1)\n",
    "    return img\n",
    "\n",
    "\n",
    "def show_gradcam(image, cam, title, save_path=None):\n",
    "    \"\"\"\n",
    "    Affiche l'image originale, la heatmap Grad-CAM, et la superposition.\n",
    "    \n",
    "    Args:\n",
    "        image: Image tensor (C, H, W)\n",
    "        cam: Heatmap Grad-CAM (H, W)\n",
    "        title: Titre pour la visualisation\n",
    "        save_path: Chemin pour sauvegarder (optionnel)\n",
    "    \"\"\"\n",
    "    # D√©normaliser l'image\n",
    "    img = denormalize_image(image)\n",
    "    \n",
    "    # Cr√©er la heatmap color√©e\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n",
    "    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB) / 255.0\n",
    "    \n",
    "    # Superposer (60% image originale + 40% heatmap)\n",
    "    overlay = 0.6 * img + 0.4 * heatmap\n",
    "    overlay = np.clip(overlay, 0, 1)\n",
    "    \n",
    "    # Afficher\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    axes[0].imshow(img)\n",
    "    axes[0].set_title('Image Originale', fontsize=13, fontweight='bold')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(cam, cmap='jet')\n",
    "    axes[1].set_title('Grad-CAM Heatmap', fontsize=13, fontweight='bold')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    axes[2].imshow(overlay)\n",
    "    axes[2].set_title(f'Superposition\\n{title}', fontsize=13, fontweight='bold')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(\"‚úÖ Fonctions Grad-CAM d√©finies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0aa2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# VISUALISATION GRAD-CAM\n",
    "# ====================================\n",
    "print(\"\\nüé® G√©n√©ration des visualisations Grad-CAM...\\n\")\n",
    "\n",
    "# R√©cup√©rer quelques exemples du test set\n",
    "model.eval()\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "num_examples = min(5, len(images))\n",
    "\n",
    "for i in range(num_examples):\n",
    "    image = images[i]\n",
    "    true_label = labels[i].item()\n",
    "    \n",
    "    # Pr√©diction\n",
    "    with torch.no_grad():\n",
    "        output = model(image.unsqueeze(0).to(device))\n",
    "        pred_label = output.argmax(1).item()\n",
    "        confidence = torch.softmax(output, dim=1)[0, pred_label].item()\n",
    "    \n",
    "    # G√©n√©rer Grad-CAM sur la classe pr√©dite\n",
    "    cam = generate_gradcam(model, image, pred_label)\n",
    "    \n",
    "    # Cr√©er le titre\n",
    "    pred_class = train_dataset.class_names[pred_label]\n",
    "    true_class = train_dataset.class_names[true_label]\n",
    "    is_correct = \"‚úÖ\" if pred_label == true_label else \"‚ùå\"\n",
    "    title = f\"{is_correct} Pred: {pred_class} ({confidence*100:.1f}%) | True: {true_class}\"\n",
    "    \n",
    "    # Afficher\n",
    "    show_gradcam(image, cam, title, save_path=f'../models/gradcam_example_{i+1}.png')\n",
    "    \n",
    "    print(f\"   {i+1}. {title}\")\n",
    "\n",
    "print(f\"\\n‚úÖ {num_examples} visualisations Grad-CAM g√©n√©r√©es et sauvegard√©es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b3dca",
   "metadata": {},
   "source": [
    "## 7. Sauvegarde des R√©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5428b392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# SAUVEGARDE DES R√âSULTATS\n",
    "# ====================================\n",
    "print(\"\\nüíæ Sauvegarde des r√©sultats finaux...\\n\")\n",
    "\n",
    "results = {\n",
    "    'model': 'CNN Custom',\n",
    "    'architecture': {\n",
    "        'blocks': 4,\n",
    "        'channels': '3‚Üí64‚Üí128‚Üí256‚Üí512',\n",
    "        'total_params': int(total_params),\n",
    "        'trainable_params': int(trainable_params)\n",
    "    },\n",
    "    'training': {\n",
    "        'epochs': num_epochs,\n",
    "        'batch_size': batch_size,\n",
    "        'optimizer': 'Adam',\n",
    "        'learning_rate': 0.001,\n",
    "        'scheduler': 'ReduceLROnPlateau'\n",
    "    },\n",
    "    'performance': {\n",
    "        'best_val_acc': float(best_val_acc),\n",
    "        'test_acc': float(test_acc),\n",
    "        'best_epoch': int(checkpoint['epoch'])\n",
    "    },\n",
    "    'dataset': {\n",
    "        'num_classes': len(train_dataset.class_names),\n",
    "        'class_names': train_dataset.class_names,\n",
    "        'train_size': len(train_dataset),\n",
    "        'val_size': len(val_dataset),\n",
    "        'test_size': len(test_dataset)\n",
    "    },\n",
    "    'history': {\n",
    "        'train_loss': [float(x) for x in history['train_loss']],\n",
    "        'train_acc': [float(x) for x in history['train_acc']],\n",
    "        'val_loss': [float(x) for x in history['val_loss']],\n",
    "        'val_acc': [float(x) for x in history['val_acc']]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Sauvegarder en JSON\n",
    "with open('../models/results_cnn_custom.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"‚úÖ R√©sultats sauvegard√©s dans: ../models/results_cnn_custom.json\")\n",
    "print(\"\\nüìä R√©sum√© Final:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"   Mod√®le: CNN Custom ({total_params:,} param√®tres)\")\n",
    "print(f\"   Meilleure Val Accuracy: {best_val_acc:.2f}% (Epoch {checkpoint['epoch']})\")\n",
    "print(f\"   Test Accuracy: {test_acc:.2f}%\")\n",
    "print(f\"   Classes: {len(train_dataset.class_names)}\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n‚úÖ Tous les r√©sultats ont √©t√© sauvegard√©s avec succ√®s!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea1030b",
   "metadata": {},
   "source": [
    "# Mod√®le 1: CNN Custom avec Explainability\n",
    "\n",
    "Architecture CNN personnalis√©e avec:\n",
    "- Plusieurs couches convolutionnelles\n",
    "- Batch Normalization\n",
    "- Dropout pour r√©gularisation\n",
    "- Grad-CAM pour l'explicabilit√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef891713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import cv2\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üñ•Ô∏è  Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66051718",
   "metadata": {},
   "source": [
    "## 1. Dataset et DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39a6f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224), interpolation=transforms.InterpolationMode.LANCZOS),\n",
    "    \n",
    "    # Geometric transforms (work on PIL Image)\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.3),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    \n",
    "    # Color/Light transforms (work on PIL Image)\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.RandomAutocontrast(p=0.3),\n",
    "    transforms.RandomAdjustSharpness(sharpness_factor=2, p=0.3),\n",
    "    \n",
    "    # GaussianBlur works on PIL\n",
    "    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 0.5)),\n",
    "    \n",
    "    # ‚úÖ CONVERT TO TENSOR FIRST (before RandomErasing!)\n",
    "    transforms.ToTensor(),\n",
    "    \n",
    "    # Noise/Texture transforms (MUST work on Tensor)\n",
    "    transforms.RandomErasing(p=0.2, scale=(0.02, 0.1)),\n",
    "    \n",
    "    # ImageNet normalization (works on Tensor)\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ff637f",
   "metadata": {},
   "source": [
    "## 2. Architecture CNN Custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e803199c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, num_classes=11):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        \n",
    "        # Block 1: 3 -> 64\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2)  # 224 -> 112\n",
    "        )\n",
    "        \n",
    "        # Block 2: 64 -> 128\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2)  # 112 -> 56\n",
    "        )\n",
    "        \n",
    "        # Block 3: 128 -> 256\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2)  # 56 -> 28\n",
    "        )\n",
    "        \n",
    "        # Block 4: 256 -> 512\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2)  # 28 -> 14\n",
    "        )\n",
    "        \n",
    "        # Global Average Pooling + Classifier\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Pour Grad-CAM\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        \n",
    "        # Sauvegarder pour Grad-CAM\n",
    "        if x.requires_grad:\n",
    "            x.register_hook(self.save_gradient)\n",
    "        self.activations = x\n",
    "        \n",
    "        x = self.gap(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "    def save_gradient(self, grad):\n",
    "        self.gradients = grad\n",
    "\n",
    "# Cr√©er le mod√®le\n",
    "model = CustomCNN(num_classes=11).to(device)\n",
    "print(\"\\nüèóÔ∏è  Architecture du mod√®le:\")\n",
    "print(model)\n",
    "\n",
    "# Compter les param√®tres\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nüìä Total param√®tres: {total_params:,}\")\n",
    "print(f\"üìä Param√®tres entra√Ænables: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537d0ab8",
   "metadata": {},
   "source": [
    "## 3. Entra√Ænement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9013f26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    \"\"\"Entra√Æner une epoch avec v√©rification des tensors\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in tqdm(loader, desc='Training'):\n",
    "        # V√©rifier que ce sont des tensors\n",
    "        if not isinstance(images, torch.Tensor):\n",
    "            raise TypeError(f\"Expected tensor, got {type(images)}\")\n",
    "        \n",
    "        # D√©placer vers device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # V√©rifier la shape\n",
    "        assert images.dim() == 4, f\"Expected 4D, got {images.dim()}D: {images.shape}\"\n",
    "        assert images.shape[1:] == (3, 224, 224), f\"Wrong shape: {images.shape}\"\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return running_loss / len(loader), 100. * correct / total\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    \"\"\"Valider avec v√©rification des tensors\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            # V√©rifier que ce sont des tensors\n",
    "            if not isinstance(images, torch.Tensor):\n",
    "                raise TypeError(f\"Expected tensor, got {type(images)}\")\n",
    "            \n",
    "            # D√©placer vers device\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # V√©rifier la shape\n",
    "            assert images.dim() == 4, f\"Expected 4D, got {images.dim()}D: {images.shape}\"\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return running_loss / len(loader), 100. * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0815e065",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================\n",
    "# FINAL CHECK before training\n",
    "# =============================================\n",
    "print(\"üéØ FINAL VERIFICATION before training...\\n\")\n",
    "\n",
    "# Test single sample from each loader\n",
    "loaders_to_test = [\n",
    "    (\"Train\", train_loader),\n",
    "    (\"Validation\", val_loader),\n",
    "    (\"Test\", test_loader)\n",
    "]\n",
    "\n",
    "for loader_name, loader in loaders_to_test:\n",
    "    images, labels = next(iter(loader))\n",
    "    \n",
    "    # Verify it's a tensor\n",
    "    assert isinstance(images, torch.Tensor), f\"{loader_name}: Not a tensor!\"\n",
    "    assert images.dim() == 4, f\"{loader_name}: Wrong dimensions!\"\n",
    "    assert images.shape[1:] == (3, 224, 224), f\"{loader_name}: Wrong shape!\"\n",
    "    \n",
    "    print(f\"‚úÖ {loader_name:12} | Shape: {images.shape} | Type: {type(images).__name__} | Range: [{images.min():.3f}, {images.max():.3f}]\")\n",
    "\n",
    "print(\"\\n‚úÖ All loaders verified and ready for training!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328b9465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entra√Æner le mod√®le\n",
    "num_epochs = 30\n",
    "best_val_acc = 0\n",
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Sauvegarder le meilleur mod√®le\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_acc': val_acc,\n",
    "        }, 'model_cnn_custom_best.pth')\n",
    "        print(f\"‚úÖ Meilleur mod√®le sauvegard√©! Val Acc: {val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb41d7bd",
   "metadata": {},
   "source": [
    "## 4. Visualisation des R√©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894519d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "axes[0].plot(history['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[0].set_title('Loss Evolution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(history['train_acc'], label='Train Accuracy', linewidth=2)\n",
    "axes[1].plot(history['val_acc'], label='Val Accuracy', linewidth=2)\n",
    "axes[1].set_title('Accuracy Evolution', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8eaa851",
   "metadata": {},
   "source": [
    "## 5. √âvaluation sur le Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e665ca02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Charger le meilleur mod√®le\n",
    "checkpoint = torch.load('model_cnn_custom_best.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# √âvaluer\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = outputs.max(1)\n",
    "        \n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nüìä Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=train_dataset.class_names))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=train_dataset.class_names,\n",
    "            yticklabels=train_dataset.class_names)\n",
    "plt.title('Confusion Matrix - CNN Custom', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d624e18d",
   "metadata": {},
   "source": [
    "## 6. Explainability: Grad-CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa80b22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gradcam(model, image, label):\n",
    "    \"\"\"G√©n√®re une heatmap Grad-CAM\"\"\"\n",
    "    model.eval()\n",
    "    image = image.unsqueeze(0).to(device)\n",
    "    image.requires_grad = True\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(image)\n",
    "    \n",
    "    # Backward pass\n",
    "    model.zero_grad()\n",
    "    output[0, label].backward()\n",
    "    \n",
    "    # Obtenir gradients et activations\n",
    "    gradients = model.gradients.cpu().data.numpy()[0]\n",
    "    activations = model.activations.cpu().data.numpy()[0]\n",
    "    \n",
    "    # Calculer les poids\n",
    "    weights = np.mean(gradients, axis=(1, 2))\n",
    "    \n",
    "    # Calculer la CAM\n",
    "    cam = np.zeros(activations.shape[1:], dtype=np.float32)\n",
    "    for i, w in enumerate(weights):\n",
    "        cam += w * activations[i]\n",
    "    \n",
    "    cam = np.maximum(cam, 0)\n",
    "    cam = cam / cam.max() if cam.max() != 0 else cam\n",
    "    cam = cv2.resize(cam, (224, 224))\n",
    "    \n",
    "    return cam\n",
    "\n",
    "def show_gradcam(image, cam, title):\n",
    "    \"\"\"Affiche l'image avec la heatmap Grad-CAM\"\"\"\n",
    "    # D√©normaliser l'image\n",
    "    img = image.cpu().numpy().transpose(1, 2, 0)\n",
    "    img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "    img = np.clip(img, 0, 1)\n",
    "    \n",
    "    # Cr√©er heatmap\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n",
    "    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB) / 255.0\n",
    "    \n",
    "    # Superposer\n",
    "    overlay = 0.6 * img + 0.4 * heatmap\n",
    "    overlay = np.clip(overlay, 0, 1)\n",
    "    \n",
    "    # Afficher\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    axes[0].imshow(img)\n",
    "    axes[0].set_title('Image Originale')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(cam, cmap='jet')\n",
    "    axes[1].set_title('Grad-CAM Heatmap')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    axes[2].imshow(overlay)\n",
    "    axes[2].set_title(f'Superposition - {title}')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2632345d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Visualiser Grad-CAM sur quelques exemples\n",
    "model.eval()\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "for i in range(min(5, len(images))):\n",
    "    image = images[i]\n",
    "    label = labels[i].item()\n",
    "    \n",
    "    # Pr√©diction\n",
    "    with torch.no_grad():\n",
    "        output = model(image.unsqueeze(0).to(device))\n",
    "        pred = output.argmax(1).item()\n",
    "    \n",
    "    # G√©n√©rer Grad-CAM\n",
    "    cam = generate_gradcam(model, image, pred)\n",
    "    \n",
    "    # Afficher\n",
    "    title = f\"Pred: {train_dataset.class_names[pred]} | True: {train_dataset.class_names[label]}\"\n",
    "    show_gradcam(image, cam, title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549fae5b",
   "metadata": {},
   "source": [
    "## 7. Sauvegarde des M√©triques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11283e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder l'historique et les m√©triques\n",
    "results = {\n",
    "    'model': 'CNN Custom',\n",
    "    'best_val_acc': float(best_val_acc),\n",
    "    'test_acc': float(100. * sum(np.array(all_preds) == np.array(all_labels)) / len(all_labels)),\n",
    "    'num_params': int(total_params),\n",
    "    'history': history\n",
    "}\n",
    "\n",
    "with open('results_cnn_custom.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"\\n‚úÖ R√©sultats sauvegard√©s dans results_cnn_custom.json\")\n",
    "print(f\"üìä Test Accuracy: {results['test_acc']:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spices_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
