{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entra√Ænement MobileNetV3 pour Classification d'√âpices Marocaines\n",
    "## Projet Master - Transfer Learning avec PyTorch\n",
    "\n",
    "**Dataset:** 2200 images, 11 classes d'√©pices marocaines  \n",
    "**Architecture:** MobileNetV3-Large (pr√©-entra√Æn√© sur ImageNet)  \n",
    "**Framework:** PyTorch avec GPU support\n",
    "\n",
    "---\n",
    "\n",
    "### üìã Plan d'Ex√©cution:\n",
    "1. Configuration et imports\n",
    "2. Chargement des statistiques EDA\n",
    "3. Configuration des DataLoaders avec augmentation\n",
    "4. Construction du mod√®le MobileNetV3\n",
    "5. Configuration de l'entra√Ænement\n",
    "6. Boucle d'entra√Ænement avec early stopping\n",
    "7. √âvaluation et m√©triques\n",
    "8. Visualisation des r√©sultats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration et Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des packages (pour Google Colab)\n",
    "# D√©commentez si vous √™tes sur Colab\n",
    "# !pip install -q torch torchvision tqdm matplotlib seaborn scikit-learn pandas\n",
    "\n",
    "#print(\"V√©rification de l'environnement...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Imports termin√©s\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import copy\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration de l'affichage\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "print(\"‚úì Imports termin√©s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CONFIGURATION MAT√âRIELLE\n",
      "================================================================================\n",
      "\n",
      "Device utilis√©: cpu\n",
      "‚ö† GPU non disponible - L'entra√Ænement sera plus lent sur CPU\n",
      "\n",
      "PyTorch version: 2.10.0+cpu\n",
      "Torchvision version: 2.10.0+cpu\n"
     ]
    }
   ],
   "source": [
    "# V√©rification GPU\n",
    "print(\"=\"*80)\n",
    "print(\"CONFIGURATION MAT√âRIELLE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nDevice utilis√©: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"M√©moire GPU disponible: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"‚ö† GPU non disponible - L'entra√Ænement sera plus lent sur CPU\")\n",
    "\n",
    "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
    "print(f\"Torchvision version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration des Chemins et Hyperparam√®tres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V√©rification des chemins:\n",
      "  Dataset: C:\\Users\\zaineb\\Desktop\\spices 5-2\\dataset\\splits - ‚úì\n",
      "  EDA Results: C:\\Users\\zaineb\\Desktop\\spices 5-2\\eda_results - ‚úì\n",
      "\n",
      "Configuration charg√©e:\n",
      "  image_size: 224\n",
      "  batch_size: 16\n",
      "  num_workers: 0\n",
      "  num_epochs: 20\n",
      "  learning_rate: 0.001\n",
      "  weight_decay: 0.0001\n",
      "  freeze_backbone: True\n",
      "  unfreeze_epoch: 5\n",
      "  unfreeze_lr: 0.0001\n",
      "  patience: 5\n",
      "  min_delta: 0.001\n",
      "  use_augmentation: True\n",
      "  seed: 42\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION - MODIFIEZ CES CHEMINS\n",
    "# ============================================================================\n",
    "\n",
    "# Chemins du dataset (chemins absolus recommand√©s)\n",
    "BASE_PATH = Path('C:/Users/zaineb/Desktop/spices 5-2')\n",
    "DATASET_PATH = BASE_PATH / 'dataset' / 'splits'  # Chemin vers votre dataset (train, val, test)\n",
    "EDA_RESULTS_PATH = BASE_PATH / 'eda_results'  # Dossier des r√©sultats EDA\n",
    "\n",
    "# Cr√©ation des dossiers de sortie\n",
    "MODELS_DIR = BASE_PATH / 'models'\n",
    "RESULTS_DIR = BASE_PATH / 'mobilenet_results'\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# V√©rifier que les dossiers de donn√©es existent\n",
    "print(\"V√©rification des chemins:\")\n",
    "print(f\"  Dataset: {DATASET_PATH} - {'‚úì' if DATASET_PATH.exists() else '‚úó'}\")\n",
    "print(f\"  EDA Results: {EDA_RESULTS_PATH} - {'‚úì' if EDA_RESULTS_PATH.exists() else '‚úó'}\")\n",
    "\n",
    "# Classes d'√©pices (11 classes - anis inclus!)\n",
    "SPICE_CLASSES = [\n",
    "    'anis', 'cannelle', 'carvi', 'clou_girofle', 'cubebe',\n",
    "    'cumin', 'curcuma', 'gingembre', 'paprika', 'poivre noir', 'safran'\n",
    "]\n",
    "\n",
    "NUM_CLASSES = len(SPICE_CLASSES)\n",
    "\n",
    "# ============================================================================\n",
    "# HYPERPARAM√àTRES\n",
    "# ============================================================================\n",
    "\n",
    "CONFIG = {\n",
    "    # Donn√©es\n",
    "    'image_size': 224,  # R√©duit √† 224 pour MobileNetV3 standard\n",
    "    'batch_size': 16,   # R√©duit pour √©viter probl√®mes m√©moire sur CPU\n",
    "    'num_workers': 0,   # 0 pour Windows (√©vite probl√®mes de multiprocessing)\n",
    "    \n",
    "    # Entra√Ænement\n",
    "    'num_epochs': 20,   # R√©duit pour d√©monstration\n",
    "    'learning_rate': 0.001,\n",
    "    'weight_decay': 1e-4,\n",
    "    \n",
    "    # Transfer Learning\n",
    "    'freeze_backbone': True,  # Geler les couches pr√©-entra√Æn√©es au d√©but\n",
    "    'unfreeze_epoch': 5,       # Epoch o√π on d√©g√®le le backbone\n",
    "    'unfreeze_lr': 0.0001,     # Learning rate apr√®s d√©gel\n",
    "    \n",
    "    # Early Stopping\n",
    "    'patience': 5,             # Nombre d'epochs sans am√©lioration\n",
    "    'min_delta': 0.001,        # Am√©lioration minimale\n",
    "    \n",
    "    # Augmentation\n",
    "    'use_augmentation': True,\n",
    "    \n",
    "    # Seed pour reproductibilit√©\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "print(\"\\nConfiguration charg√©e:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Seed fix√© √† 42 pour reproductibilit√©\n"
     ]
    }
   ],
   "source": [
    "# Seed pour reproductibilit√©\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(CONFIG['seed'])\n",
    "print(f\"‚úì Seed fix√© √† {CONFIG['seed']} pour reproductibilit√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chargement des Statistiques de Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Statistiques de normalisation charg√©es depuis l'EDA:\n",
      "  Mean: ['0.6029', '0.5535', '0.5049']\n",
      "  Std:  ['0.1871', '0.2081', '0.2310']\n"
     ]
    }
   ],
   "source": [
    "# Charger les statistiques calcul√©es lors de l'EDA\n",
    "norm_stats_path = EDA_RESULTS_PATH / 'normalization_stats.json'\n",
    "\n",
    "if norm_stats_path.exists():\n",
    "    try:\n",
    "        with open(norm_stats_path, 'r', encoding='utf-8') as f:\n",
    "            norm_stats = json.load(f)\n",
    "        \n",
    "        MEAN = [norm_stats['mean']['r'], norm_stats['mean']['g'], norm_stats['mean']['b']]\n",
    "        STD = [norm_stats['std']['r'], norm_stats['std']['g'], norm_stats['std']['b']]\n",
    "        \n",
    "        print(\"‚úì Statistiques de normalisation charg√©es depuis l'EDA:\")\n",
    "        print(f\"  Mean: {[f'{m:.4f}' for m in MEAN]}\")\n",
    "        print(f\"  Std:  {[f'{s:.4f}' for s in STD]}\")\n",
    "    except (FileNotFoundError, json.JSONDecodeError, KeyError):\n",
    "        print(\"‚ö† Erreur lors de la lecture des statistiques, utilisation des valeurs ImageNet\")\n",
    "        MEAN = [0.485, 0.456, 0.406]\n",
    "        STD = [0.229, 0.224, 0.225]\n",
    "else:\n",
    "    print(\"‚ö† Fichier de statistiques non trouv√©:\")\n",
    "    print(f\"  Chemin attendu: {norm_stats_path}\")\n",
    "    print(\"  Utilisation des valeurs ImageNet par d√©faut\")\n",
    "    MEAN = [0.485, 0.456, 0.406]\n",
    "    STD = [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cr√©ation du Dataset Custom et DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Classe SpiceDataset d√©finie\n"
     ]
    }
   ],
   "source": [
    "class SpiceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset personnalis√© pour les √©pices marocaines.\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, split='train', transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir: Chemin vers le dossier dataset_spices\n",
    "            split: 'train', 'val', ou 'test'\n",
    "            transform: Transformations √† appliquer\n",
    "        \"\"\"\n",
    "        self.root_dir = Path(root_dir) / split\n",
    "        self.transform = transform\n",
    "        self.classes = sorted(SPICE_CLASSES)\n",
    "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
    "        \n",
    "        # Collecter tous les chemins d'images\n",
    "        self.samples = []\n",
    "        for class_name in self.classes:\n",
    "            class_dir = self.root_dir / class_name\n",
    "            if not class_dir.exists():\n",
    "                continue\n",
    "            \n",
    "            for ext in ['*.jpg', '*.jpeg', '*.png']:\n",
    "                for img_path in class_dir.glob(ext):\n",
    "                    self.samples.append((str(img_path), self.class_to_idx[class_name]))\n",
    "        \n",
    "        print(f\"‚úì {split.capitalize()} set: {len(self.samples)} images charg√©es\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        \n",
    "        # Charger l'image\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # Appliquer les transformations\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "print(\"‚úì Classe SpiceDataset d√©finie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Transformations d√©finies\n",
      "\n",
      "Augmentations du training set:\n",
      "  - Random Horizontal Flip (50%)\n",
      "  - Random Vertical Flip (30%)\n",
      "  - Random Rotation (¬±20¬∞)\n",
      "  - Color Jitter\n",
      "  - Random Affine (translation, scale)\n"
     ]
    }
   ],
   "source": [
    "# D√©finir les transformations\n",
    "\n",
    "# Transformations pour l'entra√Ænement (avec augmentation)\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((CONFIG['image_size'], CONFIG['image_size'])),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.3),\n",
    "    transforms.RandomRotation(degrees=20),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=MEAN, std=STD)\n",
    "])\n",
    "\n",
    "# Transformations pour validation/test (sans augmentation)\n",
    "val_test_transforms = transforms.Compose([\n",
    "    transforms.Resize((CONFIG['image_size'], CONFIG['image_size'])),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=MEAN, std=STD)\n",
    "])\n",
    "\n",
    "print(\"‚úì Transformations d√©finies\")\n",
    "print(\"\\nAugmentations du training set:\")\n",
    "print(\"  - Random Horizontal Flip (50%)\")\n",
    "print(\"  - Random Vertical Flip (30%)\")\n",
    "print(\"  - Random Rotation (¬±20¬∞)\")\n",
    "print(\"  - Color Jitter\")\n",
    "print(\"  - Random Affine (translation, scale)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cr√©ation des datasets...\n",
      "\n",
      "‚úì Train set: 1540 images charg√©es\n",
      "‚úì Val set: 330 images charg√©es\n",
      "‚úì Test set: 330 images charg√©es\n",
      "\n",
      "‚úì Datasets cr√©√©s avec succ√®s\n",
      "  Total: 2200 images\n"
     ]
    }
   ],
   "source": [
    "# Cr√©er les datasets\n",
    "print(\"\\nCr√©ation des datasets...\\n\")\n",
    "\n",
    "train_dataset = SpiceDataset(\n",
    "    root_dir=DATASET_PATH,\n",
    "    split='train',\n",
    "    transform=train_transforms\n",
    ")\n",
    "\n",
    "val_dataset = SpiceDataset(\n",
    "    root_dir=DATASET_PATH,\n",
    "    split='val',\n",
    "    transform=val_test_transforms\n",
    ")\n",
    "\n",
    "test_dataset = SpiceDataset(\n",
    "    root_dir=DATASET_PATH,\n",
    "    split='test',\n",
    "    transform=val_test_transforms\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Datasets cr√©√©s avec succ√®s\")\n",
    "print(f\"  Total: {len(train_dataset) + len(val_dataset) + len(test_dataset)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì DataLoaders cr√©√©s\n",
      "  Train batches: 97\n",
      "  Val batches: 21\n",
      "  Test batches: 21\n"
     ]
    }
   ],
   "source": [
    "# Cr√©er les DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=CONFIG['num_workers'],\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=CONFIG['num_workers'],\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=CONFIG['num_workers'],\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "print(\"‚úì DataLoaders cr√©√©s\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches: {len(val_loader)}\")\n",
    "print(f\"  Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "V√©rification d'un batch:\n",
      "  Images shape: torch.Size([16, 3, 224, 224])\n",
      "  Labels shape: torch.Size([16])\n",
      "  Images dtype: torch.float32\n",
      "  Labels dtype: torch.int64\n",
      "  Images range: [-3.223, 2.146]\n"
     ]
    }
   ],
   "source": [
    "# V√©rifier un batch\n",
    "images, labels = next(iter(train_loader))\n",
    "print(f\"\\nV√©rification d'un batch:\")\n",
    "print(f\"  Images shape: {images.shape}\")\n",
    "print(f\"  Labels shape: {labels.shape}\")\n",
    "print(f\"  Images dtype: {images.dtype}\")\n",
    "print(f\"  Labels dtype: {labels.dtype}\")\n",
    "print(f\"  Images range: [{images.min():.3f}, {images.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Construction du Mod√®le MobileNetV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cr√©ation du mod√®le...\n",
      "\n",
      "‚úì MobileNetV3-Large charg√© avec poids ImageNet\n",
      "‚úì Backbone gel√© (sera d√©gel√© plus tard)\n",
      "‚úì Classifier modifi√© pour 11 classes\n",
      "\n",
      "‚úì Mod√®le transf√©r√© sur cpu\n"
     ]
    }
   ],
   "source": [
    "def create_mobilenetv3_model(num_classes, pretrained=True, freeze_backbone=True):\n",
    "    \"\"\"\n",
    "    Cr√©e un mod√®le MobileNetV3-Large avec transfer learning.\n",
    "    \n",
    "    Args:\n",
    "        num_classes: Nombre de classes de sortie\n",
    "        pretrained: Utiliser les poids ImageNet\n",
    "        freeze_backbone: Geler les couches pr√©-entra√Æn√©es\n",
    "    \n",
    "    Returns:\n",
    "        model: Mod√®le MobileNetV3 modifi√©\n",
    "    \"\"\"\n",
    "    # Charger MobileNetV3-Large pr√©-entra√Æn√©\n",
    "    if pretrained:\n",
    "        weights = models.MobileNet_V3_Large_Weights.IMAGENET1K_V2\n",
    "        model = models.mobilenet_v3_large(weights=weights)\n",
    "        print(\"‚úì MobileNetV3-Large charg√© avec poids ImageNet\")\n",
    "    else:\n",
    "        model = models.mobilenet_v3_large(weights=None)\n",
    "        print(\"‚úì MobileNetV3-Large initialis√© al√©atoirement\")\n",
    "    \n",
    "    # Geler le backbone si demand√©\n",
    "    if freeze_backbone:\n",
    "        for param in model.features.parameters():\n",
    "            param.requires_grad = False\n",
    "        print(\"‚úì Backbone gel√© (sera d√©gel√© plus tard)\")\n",
    "    \n",
    "    # Modifier la derni√®re couche pour nos 11 classes\n",
    "    # MobileNetV3 a un classifier avec 2 couches Linear\n",
    "    in_features = model.classifier[3].in_features\n",
    "    \n",
    "    model.classifier[3] = nn.Sequential(\n",
    "        nn.Dropout(p=0.2, inplace=True),\n",
    "        nn.Linear(in_features, num_classes)\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úì Classifier modifi√© pour {num_classes} classes\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Cr√©er le mod√®le\n",
    "print(\"\\nCr√©ation du mod√®le...\\n\")\n",
    "model = create_mobilenetv3_model(\n",
    "    num_classes=NUM_CLASSES,\n",
    "    pretrained=True,\n",
    "    freeze_backbone=CONFIG['freeze_backbone']\n",
    ")\n",
    "\n",
    "# D√©placer sur GPU\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"\\n‚úì Mod√®le transf√©r√© sur {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ARCHITECTURE DU MOD√àLE\n",
      "================================================================================\n",
      "\n",
      "Nombre total de param√®tres: 4,216,123\n",
      "Param√®tres entra√Ænables: 1,244,171\n",
      "Param√®tres gel√©s: 2,971,952\n",
      "Pourcentage entra√Ænable: 29.51%\n"
     ]
    }
   ],
   "source": [
    "# Afficher le r√©sum√© du mod√®le\n",
    "def count_parameters(model):\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total, trainable\n",
    "\n",
    "total_params, trainable_params = count_parameters(model)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ARCHITECTURE DU MOD√àLE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nNombre total de param√®tres: {total_params:,}\")\n",
    "print(f\"Param√®tres entra√Ænables: {trainable_params:,}\")\n",
    "print(f\"Param√®tres gel√©s: {total_params - trainable_params:,}\")\n",
    "print(f\"Pourcentage entra√Ænable: {100 * trainable_params / total_params:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Configuration de l'Entra√Ænement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loss, optimizer et scheduler configur√©s\n",
      "  Loss: CrossEntropyLoss\n",
      "  Optimizer: Adam (lr=0.001, wd=0.0001)\n",
      "  Scheduler: ReduceLROnPlateau (factor=0.5, patience=5)\n"
     ]
    }
   ],
   "source": [
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=CONFIG['learning_rate'],\n",
    "    weight_decay=CONFIG['weight_decay']\n",
    ")\n",
    "\n",
    "# Learning Rate Scheduler\n",
    "scheduler = ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='max',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "print(\"‚úì Loss, optimizer et scheduler configur√©s\")\n",
    "print(f\"  Loss: CrossEntropyLoss\")\n",
    "print(f\"  Optimizer: Adam (lr={CONFIG['learning_rate']}, wd={CONFIG['weight_decay']})\")\n",
    "print(f\"  Scheduler: ReduceLROnPlateau (factor=0.5, patience=5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Fonctions d'Entra√Ænement et d'√âvaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Fonctions d'entra√Ænement et early stopping d√©finies\n"
     ]
    }
   ],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    \"\"\"\n",
    "    Entra√Æne le mod√®le pour une epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc='Training', leave=False)\n",
    "    for images, labels in pbar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistiques\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        # Mise √† jour de la barre de progression\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'acc': f'{100.*correct/total:.2f}%'\n",
    "        })\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    \"\"\"\n",
    "    √âvalue le mod√®le sur le dataset de validation.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(dataloader, desc='Validation', leave=False)\n",
    "        for images, labels in pbar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{100.*correct/total:.2f}%'\n",
    "            })\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stopping pour arr√™ter l'entra√Ænement si la validation ne s'am√©liore pas.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=10, min_delta=0.001, mode='max'):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        \n",
    "    def __call__(self, score):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif self.mode == 'max':\n",
    "            if score < self.best_score + self.min_delta:\n",
    "                self.counter += 1\n",
    "                if self.counter >= self.patience:\n",
    "                    self.early_stop = True\n",
    "            else:\n",
    "                self.best_score = score\n",
    "                self.counter = 0\n",
    "        else:  # mode == 'min'\n",
    "            if score > self.best_score - self.min_delta:\n",
    "                self.counter += 1\n",
    "                if self.counter >= self.patience:\n",
    "                    self.early_stop = True\n",
    "            else:\n",
    "                self.best_score = score\n",
    "                self.counter = 0\n",
    "\n",
    "print(\"‚úì Fonctions d'entra√Ænement et early stopping d√©finies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Boucle d'Entra√Ænement Principale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "D√âBUT DE L'ENTRA√éNEMENT\n",
      "================================================================================\n",
      "\n",
      "Configuration:\n",
      "  Epochs: 20\n",
      "  Batch size: 16\n",
      "  Learning rate: 0.001\n",
      "  Early stopping patience: 5\n",
      "  Unfreeze epoch: 5\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialisation\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': [],\n",
    "    'lr': []\n",
    "}\n",
    "\n",
    "best_val_acc = 0.0\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "early_stopping = EarlyStopping(\n",
    "    patience=CONFIG['patience'],\n",
    "    min_delta=CONFIG['min_delta'],\n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"D√âBUT DE L'ENTRA√éNEMENT\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Epochs: {CONFIG['num_epochs']}\")\n",
    "print(f\"  Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"  Learning rate: {CONFIG['learning_rate']}\")\n",
    "print(f\"  Early stopping patience: {CONFIG['patience']}\")\n",
    "print(f\"  Unfreeze epoch: {CONFIG['unfreeze_epoch']}\")\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/20\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a77d91a2d2846a482538082d2f5ecce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd75cc23b5504cbf8527076235f77935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "R√©sultats:\n",
      "  Train Loss: 1.0419 | Train Acc: 66.75%\n",
      "  Val Loss:   0.4022 | Val Acc:   85.76%\n",
      "  LR: 0.001000 | Time: 201.8s\n",
      "  ‚úì Nouveau meilleur mod√®le sauvegard√©! (val_acc: 85.76%)\n",
      "\n",
      "Epoch 2/20\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "231d14afcba04a9c90bef13b21bd0ba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec3185632289485ba26342606df17952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "R√©sultats:\n",
      "  Train Loss: 0.4760 | Train Acc: 84.22%\n",
      "  Val Loss:   0.3249 | Val Acc:   91.52%\n",
      "  LR: 0.001000 | Time: 166.1s\n",
      "  ‚úì Nouveau meilleur mod√®le sauvegard√©! (val_acc: 91.52%)\n",
      "\n",
      "Epoch 3/20\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b06aea5b79484ed389a7c3e06e81cd66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81b2c8e19f5c46a98ca4d968dbbf39de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "R√©sultats:\n",
      "  Train Loss: 0.4266 | Train Acc: 85.13%\n",
      "  Val Loss:   0.3549 | Val Acc:   88.79%\n",
      "  LR: 0.001000 | Time: 203.4s\n",
      "  Early stopping: 1/5\n",
      "\n",
      "Epoch 4/20\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50cb734183c94e0bb2f933ccb748b905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65ff5cd245394cf895504108e39c29c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "R√©sultats:\n",
      "  Train Loss: 0.3570 | Train Acc: 87.79%\n",
      "  Val Loss:   0.2710 | Val Acc:   90.91%\n",
      "  LR: 0.001000 | Time: 163.1s\n",
      "  Early stopping: 2/5\n",
      "\n",
      "Epoch 5/20\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2991af32b6354709b876f6821c9cfc6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d35dd266377647078efe8c9121521714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "R√©sultats:\n",
      "  Train Loss: 0.3100 | Train Acc: 89.68%\n",
      "  Val Loss:   0.2539 | Val Acc:   92.42%\n",
      "  LR: 0.001000 | Time: 186.1s\n",
      "  ‚úì Nouveau meilleur mod√®le sauvegard√©! (val_acc: 92.42%)\n",
      "\n",
      "Epoch 6/20\n",
      "----------------------------------------\n",
      "\n",
      "üîì D√âGELAGE DU BACKBONE\n",
      "Param√®tres entra√Ænables: 4,216,123 (100.0%)\n",
      "Nouveau learning rate: 0.0001\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "337d6b545d6544b88094672ae1cee9e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87cb6bc47ae6499db75667d1b30b7fe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "R√©sultats:\n",
      "  Train Loss: 0.2341 | Train Acc: 91.23%\n",
      "  Val Loss:   0.2236 | Val Acc:   93.33%\n",
      "  LR: 0.000100 | Time: 292.7s\n",
      "  ‚úì Nouveau meilleur mod√®le sauvegard√©! (val_acc: 93.33%)\n",
      "\n",
      "Epoch 7/20\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "687857e043254b1f9e005f652437340c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccc0e52aec8044a99c62a9ca26d99b84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "R√©sultats:\n",
      "  Train Loss: 0.2058 | Train Acc: 92.99%\n",
      "  Val Loss:   0.2191 | Val Acc:   93.64%\n",
      "  LR: 0.000100 | Time: 312.7s\n",
      "  ‚úì Nouveau meilleur mod√®le sauvegard√©! (val_acc: 93.64%)\n",
      "\n",
      "Epoch 8/20\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1525c08fc7e494398835fa28c600ac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18c38adc4659481baa817bcabba27329",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "R√©sultats:\n",
      "  Train Loss: 0.2202 | Train Acc: 92.53%\n",
      "  Val Loss:   0.2171 | Val Acc:   93.94%\n",
      "  LR: 0.000100 | Time: 306.4s\n",
      "  ‚úì Nouveau meilleur mod√®le sauvegard√©! (val_acc: 93.94%)\n",
      "\n",
      "Epoch 9/20\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0545cbc4fe874557ac1aa36a8ee0c206",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e955b6e39eb4e33a21307f3f575ca3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "R√©sultats:\n",
      "  Train Loss: 0.1859 | Train Acc: 93.83%\n",
      "  Val Loss:   0.2171 | Val Acc:   94.24%\n",
      "  LR: 0.000100 | Time: 276.8s\n",
      "  ‚úì Nouveau meilleur mod√®le sauvegard√©! (val_acc: 94.24%)\n",
      "\n",
      "Epoch 10/20\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc1a4489f594471cbf328ff9ea4cde3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "797fb2adba2b43ef959fc739a8559e4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "R√©sultats:\n",
      "  Train Loss: 0.2013 | Train Acc: 92.73%\n",
      "  Val Loss:   0.2093 | Val Acc:   94.24%\n",
      "  LR: 0.000100 | Time: 261.5s\n",
      "  Early stopping: 1/5\n",
      "\n",
      "Epoch 11/20\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc058e6cd0af48e3b780cc7e3977febb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41ed33c9d0454c6d8f8b2cb8fabce0b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "R√©sultats:\n",
      "  Train Loss: 0.1773 | Train Acc: 94.61%\n",
      "  Val Loss:   0.2122 | Val Acc:   93.94%\n",
      "  LR: 0.000100 | Time: 286.6s\n",
      "  Early stopping: 2/5\n",
      "\n",
      "Epoch 12/20\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bffb37d1e9843b0bafd148b24b59b8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b5be48eb9fb463b92c419f198924b99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "R√©sultats:\n",
      "  Train Loss: 0.1613 | Train Acc: 94.29%\n",
      "  Val Loss:   0.2080 | Val Acc:   94.24%\n",
      "  LR: 0.000100 | Time: 196.0s\n",
      "  Early stopping: 3/5\n",
      "\n",
      "Epoch 13/20\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "433656db58734a3ea21fde4ab423fe70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b16deae737a046b495f475865f2c41e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "R√©sultats:\n",
      "  Train Loss: 0.1868 | Train Acc: 93.77%\n",
      "  Val Loss:   0.2057 | Val Acc:   94.85%\n",
      "  LR: 0.000100 | Time: 357.3s\n",
      "  ‚úì Nouveau meilleur mod√®le sauvegard√©! (val_acc: 94.85%)\n",
      "\n",
      "Epoch 14/20\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edaae29a6e7145bf91b8fa51b4bb9bec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# BOUCLE D'ENTRA√éNEMENT\n",
    "for epoch in range(CONFIG['num_epochs']):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1}/{CONFIG['num_epochs']}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # D√©geler le backbone apr√®s N epochs\n",
    "    if epoch == CONFIG['unfreeze_epoch'] and CONFIG['freeze_backbone']:\n",
    "        print(\"\\nüîì D√âGELAGE DU BACKBONE\")\n",
    "        for param in model.features.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        # R√©duire le learning rate\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = CONFIG['unfreeze_lr']\n",
    "        \n",
    "        total_params, trainable_params = count_parameters(model)\n",
    "        print(f\"Param√®tres entra√Ænables: {trainable_params:,} ({100*trainable_params/total_params:.1f}%)\")\n",
    "        print(f\"Nouveau learning rate: {CONFIG['unfreeze_lr']}\\n\")\n",
    "    \n",
    "    # Entra√Ænement\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Validation\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler.step(val_acc)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Sauvegarder l'historique\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['lr'].append(current_lr)\n",
    "    \n",
    "    # Afficher les r√©sultats\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    print(f\"\\nR√©sultats:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"  Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.2f}%\")\n",
    "    print(f\"  LR: {current_lr:.6f} | Time: {epoch_time:.1f}s\")\n",
    "    \n",
    "    # Sauvegarder le meilleur mod√®le\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_acc': val_acc,\n",
    "            'config': CONFIG\n",
    "        }, MODELS_DIR / 'best_model.pth')\n",
    "        print(f\"  ‚úì Nouveau meilleur mod√®le sauvegard√©! (val_acc: {val_acc:.2f}%)\")\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stopping(val_acc)\n",
    "    if early_stopping.early_stop:\n",
    "        print(f\"\\n‚ö† Early stopping d√©clench√© √† l'epoch {epoch+1}\")\n",
    "        break\n",
    "    \n",
    "    if early_stopping.counter > 0:\n",
    "        print(f\"  Early stopping: {early_stopping.counter}/{CONFIG['patience']}\")\n",
    "\n",
    "# Fin de l'entra√Ænement\n",
    "total_time = time.time() - start_time\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ENTRA√éNEMENT TERMIN√â\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTemps total: {total_time/60:.1f} minutes\")\n",
    "print(f\"Meilleure validation accuracy: {best_val_acc:.2f}%\")\n",
    "print(f\"Mod√®le sauvegard√© dans: {MODELS_DIR / 'best_model.pth'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualisation des Courbes d'Entra√Ænement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history, save_path=None):\n",
    "    \"\"\"\n",
    "    Visualise les courbes d'entra√Ænement.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    # Loss\n",
    "    axes[0].plot(epochs, history['train_loss'], 'b-', label='Train Loss', linewidth=2)\n",
    "    axes[0].plot(epochs, history['val_loss'], 'r-', label='Val Loss', linewidth=2)\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('Loss Evolution', fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy\n",
    "    axes[1].plot(epochs, history['train_acc'], 'b-', label='Train Acc', linewidth=2)\n",
    "    axes[1].plot(epochs, history['val_acc'], 'r-', label='Val Acc', linewidth=2)\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Accuracy (%)')\n",
    "    axes[1].set_title('Accuracy Evolution', fontweight='bold')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Learning Rate\n",
    "    axes[2].plot(epochs, history['lr'], 'g-', linewidth=2)\n",
    "    axes[2].set_xlabel('Epoch')\n",
    "    axes[2].set_ylabel('Learning Rate')\n",
    "    axes[2].set_title('Learning Rate Schedule', fontweight='bold')\n",
    "    axes[2].set_yscale('log')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"‚úì Graphique sauvegard√©: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Visualiser\n",
    "plot_training_history(history, save_path=RESULTS_DIR / 'training_curves.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. √âvaluation sur le Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le meilleur mod√®le\n",
    "model.load_state_dict(best_model_wts)\n",
    "model.eval()\n",
    "\n",
    "print(\"√âvaluation sur le test set...\\n\")\n",
    "\n",
    "# Pr√©dictions\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader, desc='Test'):\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        _, preds = outputs.max(1)\n",
    "        \n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "# Calculer les m√©triques\n",
    "test_acc = accuracy_score(all_labels, all_preds)\n",
    "print(f\"\\n‚úì Test Accuracy: {test_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rapport de classification\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RAPPORT DE CLASSIFICATION\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "report = classification_report(\n",
    "    all_labels,\n",
    "    all_preds,\n",
    "    labels=range(NUM_CLASSES),\n",
    "    target_names=SPICE_CLASSES,\n",
    "    digits=4,\n",
    "    zero_division=0\n",
    ")\n",
    "print(report)\n",
    "\n",
    "# Sauvegarder le rapport\n",
    "with open(RESULTS_DIR / 'classification_report.txt', 'w') as f:\n",
    "    f.write(report)\n",
    "print(f\"‚úì Rapport sauvegard√©: {RESULTS_DIR / 'classification_report.txt'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de confusion\n",
    "def plot_confusion_matrix(y_true, y_pred, classes, save_path=None):\n",
    "    \"\"\"\n",
    "    Affiche la matrice de confusion.\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Normaliser\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    \n",
    "    # Matrice brute\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=classes, yticklabels=classes,\n",
    "                ax=axes[0], cbar_kws={'label': 'Count'})\n",
    "    axes[0].set_title('Matrice de Confusion (Nombres)', fontweight='bold', fontsize=14)\n",
    "    axes[0].set_ylabel('Vraie Classe')\n",
    "    axes[0].set_xlabel('Classe Pr√©dite')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Matrice normalis√©e\n",
    "    sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues',\n",
    "                xticklabels=classes, yticklabels=classes,\n",
    "                ax=axes[1], cbar_kws={'label': 'Proportion'})\n",
    "    axes[1].set_title('Matrice de Confusion (Normalis√©e)', fontweight='bold', fontsize=14)\n",
    "    axes[1].set_ylabel('Vraie Classe')\n",
    "    axes[1].set_xlabel('Classe Pr√©dite')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"‚úì Matrice sauvegard√©e: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Afficher\n",
    "plot_confusion_matrix(\n",
    "    all_labels,\n",
    "    all_preds,\n",
    "    SPICE_CLASSES,\n",
    "    save_path=RESULTS_DIR / 'confusion_matrix.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. M√©triques Par Classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer les m√©triques par classe\n",
    "precision, recall, f1, support = precision_recall_fscore_support(\n",
    "    all_labels,\n",
    "    all_preds,\n",
    "    labels=range(NUM_CLASSES),\n",
    "    average=None,\n",
    "    zero_division=0\n",
    ")\n",
    "\n",
    "# Cr√©er un DataFrame\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Classe': SPICE_CLASSES,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': f1,\n",
    "    'Support': support\n",
    "})\n",
    "\n",
    "# Calculer l'accuracy par classe\n",
    "class_accuracies = []\n",
    "for i in range(len(SPICE_CLASSES)):\n",
    "    mask = np.array(all_labels) == i\n",
    "    if mask.sum() > 0:\n",
    "        acc = (np.array(all_preds)[mask] == i).sum() / mask.sum()\n",
    "        class_accuracies.append(acc)\n",
    "    else:\n",
    "        class_accuracies.append(0)\n",
    "\n",
    "metrics_df['Accuracy'] = class_accuracies\n",
    "\n",
    "# R√©organiser les colonnes\n",
    "metrics_df = metrics_df[['Classe', 'Accuracy', 'Precision', 'Recall', 'F1-Score', 'Support']]\n",
    "\n",
    "# Afficher\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"M√âTRIQUES PAR CLASSE\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "# Sauvegarder\n",
    "metrics_df.to_csv(RESULTS_DIR / 'per_class_metrics.csv', index=False)\n",
    "print(f\"\\n‚úì M√©triques sauvegard√©es: {RESULTS_DIR / 'per_class_metrics.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser les m√©triques par classe\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "x = np.arange(len(SPICE_CLASSES))\n",
    "width = 0.2\n",
    "\n",
    "ax.bar(x - width*1.5, metrics_df['Accuracy'], width, label='Accuracy', alpha=0.8)\n",
    "ax.bar(x - width*0.5, metrics_df['Precision'], width, label='Precision', alpha=0.8)\n",
    "ax.bar(x + width*0.5, metrics_df['Recall'], width, label='Recall', alpha=0.8)\n",
    "ax.bar(x + width*1.5, metrics_df['F1-Score'], width, label='F1-Score', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Classe d\\'√©pice', fontweight='bold')\n",
    "ax.set_ylabel('Score', fontweight='bold')\n",
    "ax.set_title('M√©triques de Performance par Classe', fontweight='bold', fontsize=14)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(SPICE_CLASSES, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "ax.set_ylim([0, 1.05])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'per_class_metrics.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úì Graphique sauvegard√©: {RESULTS_DIR / 'per_class_metrics.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Sauvegarde du Rapport Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G√©n√©rer un rapport complet\n",
    "report_text = f\"\"\"\n",
    "{'='*80}\n",
    "RAPPORT D'ENTRA√éNEMENT - MOBILENETV3\n",
    "Classification d'√âpices Marocaines\n",
    "Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "{'='*80}\n",
    "\n",
    "1. CONFIGURATION\n",
    "{'-'*80}\n",
    "Architecture: MobileNetV3-Large\n",
    "Transfer Learning: ImageNet pr√©-entra√Æn√©\n",
    "Nombre de classes: {NUM_CLASSES}\n",
    "Classes: {', '.join(SPICE_CLASSES)}\n",
    "\n",
    "Hyperparam√®tres:\n",
    "  - Image size: {CONFIG['image_size']}√ó{CONFIG['image_size']}\n",
    "  - Batch size: {CONFIG['batch_size']}\n",
    "  - Initial learning rate: {CONFIG['learning_rate']}\n",
    "  - Weight decay: {CONFIG['weight_decay']}\n",
    "  - Epochs trained: {len(history['train_loss'])}\n",
    "  - Total parameters: {total_params:,}\n",
    "  - Trainable parameters: {trainable_params:,}\n",
    "\n",
    "2. DONN√âES\n",
    "{'-'*80}\n",
    "Train set: {len(train_dataset)} images\n",
    "Validation set: {len(val_dataset)} images\n",
    "Test set: {len(test_dataset)} images\n",
    "Total: {len(train_dataset) + len(val_dataset) + len(test_dataset)} images\n",
    "\n",
    "Augmentation (training):\n",
    "  - Random Horizontal Flip (50%)\n",
    "  - Random Vertical Flip (30%)\n",
    "  - Random Rotation (¬±20¬∞)\n",
    "  - Color Jitter\n",
    "  - Random Affine\n",
    "\n",
    "3. R√âSULTATS D'ENTRA√éNEMENT\n",
    "{'-'*80}\n",
    "Meilleure validation accuracy: {best_val_acc:.2f}%\n",
    "Test accuracy: {test_acc*100:.2f}%\n",
    "Temps total d'entra√Ænement: {total_time/60:.1f} minutes\n",
    "\n",
    "Derni√®re epoch:\n",
    "  - Train Loss: {history['train_loss'][-1]:.4f}\n",
    "  - Train Acc: {history['train_acc'][-1]:.2f}%\n",
    "  - Val Loss: {history['val_loss'][-1]:.4f}\n",
    "  - Val Acc: {history['val_acc'][-1]:.2f}%\n",
    "\n",
    "4. M√âTRIQUES GLOBALES (TEST SET)\n",
    "{'-'*80}\n",
    "\"\"\"\n",
    "\n",
    "# Ajouter les m√©triques moyennes\n",
    "report_text += f\"\"\"Precision moyenne: {precision.mean():.4f}\n",
    "Recall moyen: {recall.mean():.4f}\n",
    "F1-Score moyen: {f1.mean():.4f}\n",
    "\n",
    "5. TOP ET BOTTOM CLASSES\n",
    "{'-'*80}\n",
    "\"\"\"\n",
    "\n",
    "# Top 3 classes\n",
    "top_3 = metrics_df.nlargest(3, 'F1-Score')\n",
    "report_text += \"\\nMeilleures performances (F1-Score):\\n\"\n",
    "for idx, row in top_3.iterrows():\n",
    "    report_text += f\"  {idx+1}. {row['Classe']}: {row['F1-Score']:.4f}\\n\"\n",
    "\n",
    "# Bottom 3 classes\n",
    "bottom_3 = metrics_df.nsmallest(3, 'F1-Score')\n",
    "report_text += \"\\nPerformances les plus faibles (F1-Score):\\n\"\n",
    "for idx, row in bottom_3.iterrows():\n",
    "    report_text += f\"  {idx+1}. {row['Classe']}: {row['F1-Score']:.4f}\\n\"\n",
    "\n",
    "report_text += f\"\"\"\n",
    "6. FICHIERS G√âN√âR√âS\n",
    "{'-'*80}\n",
    "Mod√®le: {MODELS_DIR / 'best_model.pth'}\n",
    "Courbes d'entra√Ænement: {RESULTS_DIR / 'training_curves.png'}\n",
    "Matrice de confusion: {RESULTS_DIR / 'confusion_matrix.png'}\n",
    "Rapport de classification: {RESULTS_DIR / 'classification_report.txt'}\n",
    "M√©triques par classe: {RESULTS_DIR / 'per_class_metrics.csv'}\n",
    "\n",
    "{'='*80}\n",
    "FIN DU RAPPORT\n",
    "{'='*80}\n",
    "\"\"\"\n",
    "\n",
    "# Sauvegarder\n",
    "with open(RESULTS_DIR / 'training_report.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(report_text)\n",
    "\n",
    "print(report_text)\n",
    "print(f\"\\n‚úì Rapport complet sauvegard√©: {RESULTS_DIR / 'training_report.txt'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Fonction de Pr√©diction pour de Nouvelles Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(image_path, model, transform, device, classes):\n",
    "    \"\"\"\n",
    "    Pr√©dit la classe d'une nouvelle image.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Chemin vers l'image\n",
    "        model: Mod√®le entra√Æn√©\n",
    "        transform: Transformations √† appliquer\n",
    "        device: CPU ou GPU\n",
    "        classes: Liste des noms de classes\n",
    "    \n",
    "    Returns:\n",
    "        predicted_class: Classe pr√©dite\n",
    "        confidence: Confiance de la pr√©diction\n",
    "        all_probs: Probabilit√©s pour toutes les classes\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Charger et pr√©parer l'image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Pr√©diction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image_tensor)\n",
    "        probabilities = torch.softmax(outputs, dim=1)\n",
    "        confidence, predicted = probabilities.max(1)\n",
    "    \n",
    "    predicted_class = classes[predicted.item()]\n",
    "    confidence = confidence.item()\n",
    "    all_probs = probabilities.cpu().numpy()[0]\n",
    "    \n",
    "    return predicted_class, confidence, all_probs\n",
    "\n",
    "\n",
    "# Exemple d'utilisation\n",
    "def visualize_prediction(image_path, model, transform, device, classes):\n",
    "    \"\"\"\n",
    "    Visualise une pr√©diction.\n",
    "    \"\"\"\n",
    "    predicted_class, confidence, all_probs = predict_image(\n",
    "        image_path, model, transform, device, classes\n",
    "    )\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Image\n",
    "    image = Image.open(image_path)\n",
    "    axes[0].imshow(image)\n",
    "    axes[0].axis('off')\n",
    "    axes[0].set_title(f'Pr√©diction: {predicted_class}\\nConfiance: {confidence*100:.2f}%',\n",
    "                     fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Barplot des probabilit√©s\n",
    "    axes[1].barh(classes, all_probs, color='skyblue')\n",
    "    axes[1].set_xlabel('Probabilit√©')\n",
    "    axes[1].set_title('Probabilit√©s par Classe', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_xlim([0, 1])\n",
    "    axes[1].grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return predicted_class, confidence\n",
    "\n",
    "print(\"‚úì Fonctions de pr√©diction d√©finies\")\n",
    "print(\"\\nPour pr√©dire une nouvelle image, utilisez:\")\n",
    "print(\"  visualize_prediction('chemin/vers/image.jpg', model, val_test_transforms, device, SPICE_CLASSES)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. R√©sum√© Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úì ENTRA√éNEMENT ET √âVALUATION TERMIN√âS AVEC SUCC√àS!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä R√©sultats Finaux:\")\n",
    "print(f\"  ‚Ä¢ Meilleure Val Accuracy: {best_val_acc:.2f}%\")\n",
    "print(f\"  ‚Ä¢ Test Accuracy: {test_acc*100:.2f}%\")\n",
    "print(f\"  ‚Ä¢ Temps d'entra√Ænement: {total_time/60:.1f} minutes\")\n",
    "\n",
    "print(f\"\\nüìÅ Fichiers g√©n√©r√©s dans {RESULTS_DIR}/:\")\n",
    "for file in sorted(RESULTS_DIR.glob('*')):\n",
    "    print(f\"  ‚Ä¢ {file.name}\")\n",
    "\n",
    "print(f\"\\nüíæ Mod√®le sauvegard√©:\")\n",
    "print(f\"  ‚Ä¢ {MODELS_DIR / 'best_model.pth'}\")\n",
    "\n",
    "print(\"\\nüéØ Prochaines √©tapes sugg√©r√©es:\")\n",
    "print(\"  1. Analyser la matrice de confusion pour identifier les confusions\")\n",
    "print(\"  2. Tester le mod√®le sur de nouvelles images\")\n",
    "print(\"  3. Exporter le mod√®le pour d√©ploiement (ONNX, TorchScript)\")\n",
    "print(\"  4. Cr√©er une application de d√©monstration\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spices_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
