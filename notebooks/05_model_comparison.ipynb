{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c022ea75",
   "metadata": {},
   "source": [
    "# Comparaison des 3 Mod√®les\n",
    "\n",
    "Analyse comparative des performances:\n",
    "1. CNN Custom\n",
    "2. ResNet-50\n",
    "3. EfficientNet-B3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20b0538",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999d7a6a",
   "metadata": {},
   "source": [
    "## 1. Charger les R√©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fd0369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les r√©sultats des trois mod√®les\n",
    "results = {}\n",
    "base_path = Path('../models')\n",
    "result_files = {\n",
    "    'CNN Custom': base_path / 'results_cnn_custom.json',\n",
    "    'ResNet-50': base_path / 'results_resnet.json',\n",
    "    'EfficientNet-B3': base_path / 'results_efficientnet.json'\n",
    "}\n",
    "\n",
    "for model_name, file_path in result_files.items():\n",
    "    if file_path.exists():\n",
    "        with open(file_path, 'r') as f:\n",
    "            results[model_name] = json.load(f)\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Fichier {file_path} non trouv√©\")\n",
    "\n",
    "print(f\"‚úÖ R√©sultats charg√©s pour {len(results)} mod√®les\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f2f943",
   "metadata": {},
   "source": [
    "## 2. Tableau Comparatif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fce899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er tableau comparatif\n",
    "comparison_data = []\n",
    "\n",
    "for model_name, result in results.items():\n",
    "    comparison_data.append({\n",
    "        'Mod√®le': model_name,\n",
    "        'Val Accuracy (%)': f\"{result['best_val_acc']:.2f}\",\n",
    "        'Test Accuracy (%)': f\"{result['test_acc']:.2f}\",\n",
    "        'Param√®tres (M)': f\"{result['num_params'] / 1e6:.2f}\",\n",
    "    })\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "print(\"\\nüìä Comparaison des Mod√®les:\")\n",
    "print(df_comparison.to_string(index=False))\n",
    "\n",
    "# Identifier le meilleur\n",
    "best_model = max(results.items(), key=lambda x: x[1]['test_acc'])\n",
    "print(f\"\\nüèÜ Meilleur mod√®le: {best_model[0]} avec {best_model[1]['test_acc']:.2f}% de test accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42c3a36",
   "metadata": {},
   "source": [
    "## 3. Visualisations Comparatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb68e8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison des accuracies\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Val Accuracy\n",
    "val_accs = [results[m]['best_val_acc'] for m in results.keys()]\n",
    "test_accs = [results[m]['test_acc'] for m in results.keys()]\n",
    "model_names = list(results.keys())\n",
    "\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x - width/2, val_accs, width, label='Val Accuracy', color='skyblue')\n",
    "axes[0].bar(x + width/2, test_accs, width, label='Test Accuracy', color='lightcoral')\n",
    "axes[0].set_ylabel('Accuracy (%)')\n",
    "axes[0].set_title('Comparaison des Performances', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(model_names, rotation=15)\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Nombre de param√®tres\n",
    "params = [results[m]['num_params'] / 1e6 for m in results.keys()]\n",
    "colors = sns.color_palette(\"husl\", len(model_names))\n",
    "axes[1].bar(model_names, params, color=colors)\n",
    "axes[1].set_ylabel('Param√®tres (Millions)')\n",
    "axes[1].set_title('Complexit√© des Mod√®les', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xticklabels(model_names, rotation=15)\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1e0b22",
   "metadata": {},
   "source": [
    "## 4. Courbes d'Apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482ec792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparer les courbes d'apprentissage\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss\n",
    "for model_name, result in results.items():\n",
    "    history = result['history']\n",
    "    axes[0].plot(history['val_loss'], label=model_name, linewidth=2)\n",
    "\n",
    "axes[0].set_title('Validation Loss Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "for model_name, result in results.items():\n",
    "    history = result['history']\n",
    "    axes[1].plot(history['val_acc'], label=model_name, linewidth=2)\n",
    "\n",
    "axes[1].set_title('Validation Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a2d420",
   "metadata": {},
   "source": [
    "## 5. Analyse Performance vs Complexit√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfa2fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot: Test Accuracy vs Nombre de param√®tres\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for model_name, result in results.items():\n",
    "    params = result['num_params'] / 1e6\n",
    "    test_acc = result['test_acc']\n",
    "    plt.scatter(params, test_acc, s=200, label=model_name, alpha=0.7)\n",
    "    plt.annotate(model_name, (params, test_acc), \n",
    "                xytext=(10, 5), textcoords='offset points',\n",
    "                fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.xlabel('Nombre de Param√®tres (Millions)', fontsize=12)\n",
    "plt.ylabel('Test Accuracy (%)', fontsize=12)\n",
    "plt.title('Performance vs Complexit√©', fontsize=14, fontweight='bold')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26995e87",
   "metadata": {},
   "source": [
    "## 6. Recommandations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dafaf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìã RECOMMANDATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Trouver le meilleur par crit√®re\n",
    "best_accuracy = max(results.items(), key=lambda x: x[1]['test_acc'])\n",
    "smallest_model = min(results.items(), key=lambda x: x[1]['num_params'])\n",
    "\n",
    "# Calculer efficiency score (accuracy / params)\n",
    "efficiency = {}\n",
    "for model_name, result in results.items():\n",
    "    efficiency[model_name] = result['test_acc'] / (result['num_params'] / 1e6)\n",
    "best_efficiency = max(efficiency.items(), key=lambda x: x[1])\n",
    "\n",
    "print(f\"\\nüèÜ Meilleure Accuracy:\")\n",
    "print(f\"   {best_accuracy[0]}: {best_accuracy[1]['test_acc']:.2f}%\")\n",
    "\n",
    "print(f\"\\n‚ö° Mod√®le le Plus L√©ger:\")\n",
    "print(f\"   {smallest_model[0]}: {smallest_model[1]['num_params']/1e6:.2f}M param√®tres\")\n",
    "\n",
    "print(f\"\\nüíé Meilleur Rapport Performance/Taille:\")\n",
    "print(f\"   {best_efficiency[0]}: {best_efficiency[1]:.2f} acc/M_params\")\n",
    "\n",
    "print(\"\\nüì± Pour d√©ploiement mobile:\")\n",
    "print(\"   ‚Üí Privil√©gier EfficientNet-B3 (l√©ger et performant)\")\n",
    "\n",
    "print(\"\\nüñ•Ô∏è  Pour serveur avec GPU:\")\n",
    "print(f\"   ‚Üí Utiliser {best_accuracy[0]} (meilleure accuracy)\")\n",
    "\n",
    "print(\"\\nüî¨ Pour recherche/exp√©rimentation:\")\n",
    "print(\"   ‚Üí CNN Custom (plus flexible et modifiable)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fca9b3",
   "metadata": {},
   "source": [
    "## 7. Export du Rapport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e382ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er rapport final\n",
    "report = {\n",
    "    'comparison': comparison_data,\n",
    "    'best_accuracy': {\n",
    "        'model': best_accuracy[0],\n",
    "        'accuracy': float(best_accuracy[1]['test_acc'])\n",
    "    },\n",
    "    'smallest_model': {\n",
    "        'model': smallest_model[0],\n",
    "        'params': int(smallest_model[1]['num_params'])\n",
    "    },\n",
    "    'best_efficiency': {\n",
    "        'model': best_efficiency[0],\n",
    "        'score': float(best_efficiency[1])\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('model_comparison_report.json', 'w') as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Rapport de comparaison sauvegard√©: model_comparison_report.json\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
